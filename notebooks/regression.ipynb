{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5213bb4",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red;\">Section 1: Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8be6f",
   "metadata": {},
   "source": [
    "<h2>1) Préparation de données</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f748d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(44)  # à chaque exécution, générer le même dataset de manière aléatoire\n",
    "\n",
    "# Coefficients\n",
    "a1, a2, b = 2, 3, 5  # y = 2*X1 + 3*X2 + 5 + bruit\n",
    "nombre_points = 100  # Nombre de points\n",
    "\n",
    "# Génération des deux features (X1 et X2)\n",
    "X1 = np.random.rand(nombre_points) * 10\n",
    "X2 = np.random.rand(nombre_points) * 10\n",
    "\n",
    "# Empilement des features dans une seule matrice (shape: (100, 2))\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "# Génération du bruit\n",
    "bruit = np.random.randn(nombre_points) * 2  # Bruit\n",
    "\n",
    "# Calcul de la target\n",
    "y = a1 * X1 + a2 * X2 + b + bruit\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "importance_split",
   "metadata": {},
   "source": [
    "### Importance de la division en training set et test set\n",
    "\n",
    "La division des données en ensembles d'entraînement et de test est cruciale pour :\n",
    "1. **Évaluer la performance réelle** : Le test set permet d'évaluer comment le modèle généralise sur des données non vues\n",
    "2. **Détecter le surapprentissage** : Si le modèle performe bien sur les données d'entraînement mais mal sur le test, il y a surapprentissage\n",
    "3. **Sélection de modèles** : Comparer différents modèles de manière objective\n",
    "4. **Validation des hyperparamètres** : Optimiser les paramètres sans biaiser l'évaluation finale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52055be8",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red;\">Section 2: Neural network avec tensorflow</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207be9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a611a",
   "metadata": {},
   "source": [
    "<h2>2) Modèle de réseau de neurones</h2>\n",
    "\n",
    "### Architecture proposée:\n",
    "- **Inputs**: 2 features (X1, X2)\n",
    "- **Couches cachées**: 0 (réseau simple)\n",
    "- **Couche de sortie**: 1 neurone (régression)\n",
    "- **Fonction d'activation**: linéaire (pas d'activation pour la régression)\n",
    "- **Nombre de paramètres**: 2 poids + 1 biais = 3 paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "model_nn = Sequential()\n",
    "output_layer = Dense(1, input_shape=(X_train.shape[1],), activation='linear')\n",
    "model_nn.add(output_layer)\n",
    "\n",
    "# Configuration de l'optimiseur\n",
    "opt = Adam(learning_rate=0.01)\n",
    "\n",
    "# Compilation du modèle\n",
    "model_nn.compile(optimizer=opt, loss=\"mse\", metrics=[\"mse\"])\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model_nn.summary()\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model_nn.fit(X_train, y_train, epochs=4100, verbose=0, validation_split=0.2)\n",
    "\n",
    "print(\"\\nEntraînement terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "function_fit_explanation",
   "metadata": {},
   "source": [
    "### Que fait la fonction fit()?\n",
    "\n",
    "La fonction `fit()` :\n",
    "1. **Entraîne le modèle** en ajustant les poids et biais\n",
    "2. **Minimise la fonction de coût** (MSE) par descente de gradient\n",
    "3. **Itère sur les epochs** : répète le processus d'apprentissage\n",
    "4. **Met à jour les paramètres** à chaque batch\n",
    "5. **Retourne l'historique** des métriques d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul manuel du nombre de paramètres\n",
    "print(\"=== ANALYSE DES PARAMÈTRES ===\")\n",
    "print(f\"Nombre d'inputs: {X_train.shape[1]}\")\n",
    "print(f\"Nombre de neurones en sortie: 1\")\n",
    "print(f\"Nombre de poids: {X_train.shape[1]} × 1 = {X_train.shape[1]}\")\n",
    "print(f\"Nombre de biais: 1\")\n",
    "print(f\"Total paramètres (calcul manuel): {X_train.shape[1] + 1}\")\n",
    "print(f\"Total paramètres (modèle): {model_nn.count_params()}\")\n",
    "\n",
    "# Extraction des paramètres\n",
    "W_nn, bias_nn = model_nn.layers[0].get_weights()\n",
    "print(f\"\\nPoids (W): {W_nn.flatten()}\")\n",
    "print(f\"Biais (b): {bias_nn}\")\n",
    "print(f\"\\nCoefficients théoriques: a1={a1}, a2={a2}, b={b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_importance",
   "metadata": {},
   "source": [
    "### Importance du tuning (régularisation)\n",
    "\n",
    "Le tuning est important dans les cas suivants :\n",
    "1. **Surapprentissage** : Le modèle mémorise les données d'entraînement\n",
    "2. **Sous-apprentissage** : Le modèle est trop simple pour capturer les patterns\n",
    "3. **Données bruitées** : Nécessité de régularisation L1/L2\n",
    "4. **Datasets complexes** : Optimisation des hyperparamètres\n",
    "5. **Contraintes computationnelles** : Équilibrer performance et vitesse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cd5cd",
   "metadata": {},
   "source": [
    "<h2>3) Prédiction en utilisant le modèle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd76e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions avec le modèle\n",
    "yhat_nn = model_nn.predict(X_test)\n",
    "yhat_nn = yhat_nn.flatten()\n",
    "\n",
    "print(f\"Forme des prédictions: {yhat_nn.shape}\")\n",
    "print(f\"Premières prédictions: {yhat_nn[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction manuelle sans utiliser predict()\n",
    "W_manual, bias_manual = model_nn.layers[0].get_weights()\n",
    "yhat_manual = X_test @ W_manual.flatten() + bias_manual[0]\n",
    "\n",
    "print(\"=== COMPARAISON PRÉDICTIONS ===\")\n",
    "print(f\"Prédictions model.predict(): {yhat_nn[:3]}\")\n",
    "print(f\"Prédictions manuelles: {yhat_manual[:3]}\")\n",
    "print(f\"Différence maximale: {np.max(np.abs(yhat_nn - yhat_manual))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503925c",
   "metadata": {},
   "source": [
    "<h2>4) Evaluation du modèle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance sur le test set\n",
    "mse_nn = mean_squared_error(y_test, yhat_nn)\n",
    "r2_nn = r2_score(y_test, yhat_nn)\n",
    "\n",
    "print(\"=== PERFORMANCE MODÈLE NEURAL ===\")\n",
    "print(f\"MSE: {mse_nn:.4f}\")\n",
    "print(f\"R²: {r2_nn:.4f}\")\n",
    "\n",
    "# Performance sur le training set\n",
    "yhat_train = model_nn.predict(X_train).flatten()\n",
    "mse_train = mean_squared_error(y_train, yhat_train)\n",
    "r2_train = r2_score(y_train, yhat_train)\n",
    "\n",
    "print(f\"\\nMSE Training: {mse_train:.4f}\")\n",
    "print(f\"R² Training: {r2_train:.4f}\")\n",
    "\n",
    "# Analyse des résidus\n",
    "residuals = y_test - yhat_nn\n",
    "\n",
    "# Graphique des résidus\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(yhat_nn, residuals, alpha=0.6, color='blue')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Valeurs prédites (ŷ)')\n",
    "plt.ylabel('Résidus (y - ŷ)')\n",
    "plt.title('Graphique des résidus')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(y_test, yhat_nn, alpha=0.6, color='green')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Valeurs réelles')\n",
    "plt.ylabel('Valeurs prédites')\n",
    "plt.title('Prédictions vs Réalité')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuals, bins=20, alpha=0.7, color='purple')\n",
    "plt.xlabel('Résidus')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des résidus')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation_results",
   "metadata": {},
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "1. **R² proche de 1** : Le modèle explique bien la variance des données\n",
    "2. **MSE faible** : Les erreurs de prédiction sont petites\n",
    "3. **Résidus aléatoires** : Pas de pattern visible = bon modèle\n",
    "4. **Performance train vs test** : Similaire = pas de surapprentissage\n",
    "\n",
    "### Pourquoi évaluer sur training ET test set ?\n",
    "- **Détecter le surapprentissage** : Performance train >> test\n",
    "- **Valider la généralisation** : Performance similaire = bon modèle\n",
    "- **Diagnostiquer les problèmes** : Sous-apprentissage si les deux sont mauvais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d59eb7",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red;\">Section 3: Régression linéaire from scratch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016264c",
   "metadata": {},
   "source": [
    "<h2>Modèle de régression linéaire from scratch avec utilisation des matrices</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calculs_elementaires",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Calculs élémentaires pour une itération\n",
    "print(\"=== CALCULS ÉLÉMENTAIRES ===\")\n",
    "\n",
    "# Dataset d'exemple\n",
    "X_example = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y_example = np.array([[10], [20], [30], [40]])\n",
    "\n",
    "# Modèle initial\n",
    "W_example = np.array([[0.2], [0.3]])\n",
    "bias_example = 0.1\n",
    "learning_rate_example = 0.01\n",
    "\n",
    "print(f\"X: \\n{X_example}\")\n",
    "print(f\"y: \\n{y_example.flatten()}\")\n",
    "print(f\"W initial: \\n{W_example.flatten()}\")\n",
    "print(f\"bias initial: {bias_example}\")\n",
    "\n",
    "# a) Calcul de la prédiction\n",
    "y_pred_example = X_example @ W_example + bias_example\n",
    "print(f\"\\na) Prédictions: \\n{y_pred_example.flatten()}\")\n",
    "\n",
    "# b) Calcul des erreurs\n",
    "errors_example = y_pred_example - y_example\n",
    "print(f\"\\nb) Erreurs: \\n{errors_example.flatten()}\")\n",
    "\n",
    "# c) Calcul des gradients\n",
    "n_example = len(X_example)\n",
    "dW_example = (1/n_example) * (X_example.T @ errors_example)\n",
    "db_example = (1/n_example) * np.sum(errors_example)\n",
    "print(f\"\\nc) Gradients:\")\n",
    "print(f\"dW: \\n{dW_example.flatten()}\")\n",
    "print(f\"db: {db_example}\")\n",
    "\n",
    "# d) Mise à jour du modèle\n",
    "W_new = W_example - learning_rate_example * dW_example\n",
    "bias_new = bias_example - learning_rate_example * db_example\n",
    "print(f\"\\nd) Nouveaux paramètres:\")\n",
    "print(f\"W nouveau: \\n{W_new.flatten()}\")\n",
    "print(f\"bias nouveau: {bias_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea5d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Entraînement complet du modèle from scratch\n",
    "print(\"\\n=== ENTRAÎNEMENT FROM SCRATCH ===\")\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000\n",
    "\n",
    "# Initialisation des paramètres\n",
    "W = np.array([[0.0], [0.0]])  # Shape: (2, 1)\n",
    "b = 0.0\n",
    "\n",
    "# Reshape y_train pour garantir les dimensions adéquates \n",
    "y_train_reshaped = y_train.reshape(-1, 1)  # Shape: (n_samples, 1)\n",
    "n = len(X_train)\n",
    "\n",
    "# Stockage des coûts pour visualisation\n",
    "costs = []\n",
    "\n",
    "# Entraînement (descente de gradient vectorisée)\n",
    "for epoch in range(epochs):\n",
    "    # Prédiction\n",
    "    y_pred = X_train @ W + b  # Shape: (n, 1)\n",
    "    \n",
    "    # Calcul de l'erreur\n",
    "    error = y_pred - y_train_reshaped  # Shape: (n, 1)\n",
    "    \n",
    "    # Calcul du coût (MSE)\n",
    "    cost = (1/(2*n)) * np.sum(error**2)\n",
    "    costs.append(cost)\n",
    "\n",
    "    # Calcul des gradients\n",
    "    dW = (1/n) * (X_train.T @ error)  # Shape: (2, 1)\n",
    "    db = (1/n) * np.sum(error)        # Scalaire\n",
    "\n",
    "    # Mise à jour des paramètres\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Cost: {cost:.6f}\")\n",
    "\n",
    "# Résultats finaux\n",
    "print(f\"\\nParamètres ajustés:\")\n",
    "print(f\"W = \\n{W}\")\n",
    "print(f\"b = {b:.4f}\")\n",
    "print(f\"\\nCoefficients théoriques: a1={a1}, a2={a2}, b={b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Évaluation du modèle from scratch\n",
    "print(\"=== ÉVALUATION MODÈLE FROM SCRATCH ===\")\n",
    "\n",
    "# Prédictions sur le test set\n",
    "yhat_scratch = X_test @ W + b\n",
    "yhat_scratch = yhat_scratch.flatten()\n",
    "\n",
    "# Métriques\n",
    "mse_scratch = mean_squared_error(y_test, yhat_scratch)\n",
    "r2_scratch = r2_score(y_test, yhat_scratch)\n",
    "\n",
    "print(f\"MSE (from scratch): {mse_scratch:.4f}\")\n",
    "print(f\"R² (from scratch): {r2_scratch:.4f}\")\n",
    "\n",
    "# Comparaison avec sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_sklearn = LinearRegression()\n",
    "lr_sklearn.fit(X_train, y_train)\n",
    "yhat_sklearn = lr_sklearn.predict(X_test)\n",
    "\n",
    "mse_sklearn = mean_squared_error(y_test, yhat_sklearn)\n",
    "r2_sklearn = r2_score(y_test, yhat_sklearn)\n",
    "\n",
    "print(f\"\\n=== COMPARAISON ===\")\n",
    "print(f\"MSE - Neural Network: {mse_nn:.4f}\")\n",
    "print(f\"MSE - From Scratch: {mse_scratch:.4f}\")\n",
    "print(f\"MSE - Sklearn: {mse_sklearn:.4f}\")\n",
    "print(f\"\\nR² - Neural Network: {r2_nn:.4f}\")\n",
    "print(f\"R² - From Scratch: {r2_scratch:.4f}\")\n",
    "print(f\"R² - Sklearn: {r2_sklearn:.4f}\")\n",
    "\n",
    "# Visualisation de la convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(costs)\n",
    "plt.title('Convergence du coût')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Cost')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, yhat_scratch, alpha=0.6, label='From Scratch', color='red')\n",
    "plt.scatter(y_test, yhat_sklearn, alpha=0.6, label='Sklearn', color='blue')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valeurs réelles')\n",
    "plt.ylabel('Valeurs prédites')\n",
    "plt.title('Comparaison des modèles')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}