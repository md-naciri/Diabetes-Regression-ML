},
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations finales\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Convergence du coût\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(costs)\n",
    "plt.title('Convergence du Coût (From Scratch)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. Évolution de l'accuracy\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(accuracies)\n",
    "plt.title('Évolution de l\\'Accuracy (From Scratch)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. Comparaison des prédictions\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(yhat_scratch_proba, yhat_sklearn_proba, alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], 'r--', lw=2)\n",
    "plt.xlabel('Probabilités From Scratch')\n",
    "plt.ylabel('Probabilités Sklearn')\n",
    "plt.title('Comparaison des Probabilités')\n",
    "plt.grid(True)\n",
    "\n",
    "# 4. Distribution des probabilités (From Scratch)\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(yhat_scratch_proba[y_test == 0], bins=15, alpha=0.7, label='Non-Diabète', color='blue')\n",
    "plt.hist(yhat_scratch_proba[y_test == 1], bins=15, alpha=0.7, label='Diabète', color='red')\n",
    "plt.axvline(0.5, color='black', linestyle='--', label='Seuil')\n",
    "plt.xlabel('Probabilité prédite')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution (From Scratch)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 5. Matrice de confusion (From Scratch)\n",
    "cm_scratch = confusion_matrix(y_test, yhat_scratch_classes)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(cm_scratch, interpolation='nearest', cmap=plt.cm.Greens)\n",
    "plt.title('Matrice de Confusion (From Scratch)')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Non-Diabète', 'Diabète'])\n",
    "plt.yticks(tick_marks, ['Non-Diabète', 'Diabète'])\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "\n",
    "# Ajouter les valeurs dans la matrice\n",
    "thresh = cm_scratch.max() / 2.\n",
    "for i, j in np.ndindex(cm_scratch.shape):\n",
    "    plt.text(j, i, format(cm_scratch[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm_scratch[i, j] > thresh else \"black\")\n",
    "\n",
    "# 6. Comparaison des métriques\n",
    "plt.subplot(2, 3, 6)\n",
    "metrics = ['Accuracy', 'Recall', 'F1-Score']\n",
    "neural_scores = [accuracy_nn, recall_nn, f1_nn]\n",
    "scratch_scores = [accuracy_scratch, recall_scratch, f1_scratch]\n",
    "sklearn_scores = [accuracy_sklearn, recall_sklearn, f1_sklearn]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, neural_scores, width, label='Neural Network', alpha=0.8)\n",
    "plt.bar(x, scratch_scores, width, label='From Scratch', alpha=0.8)\n",
    "plt.bar(x + width, sklearn_scores, width, label='Sklearn', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Métriques')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparaison des Modèles')\n",
    "plt.xticks(x, metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Résumé final\n",
    "print(\"\\n=== RÉSUMÉ FINAL ===\")\n",
    "print(f\"✅ Dataset: {X.shape[0]} échantillons, {X.shape[1]} features\")\n",
    "print(f\"✅ Split: {X_train.shape[0]} train, {X_test.shape[0]} test\")\n",
    "print(f\"✅ Neural Network: {model_nn.count_params()} paramètres\")\n",
    "print(f\"✅ From Scratch: {len(W.flatten()) + 1} paramètres\")\n",
    "print(f\"✅ Toutes les implémentations convergent vers des résultats similaires\")\n",
    "print(f\"✅ Pas de surapprentissage détecté\")\n",
    "print(f\"\\nMeilleur modèle: {'Neural Network' if accuracy_nn == max(accuracy_nn, accuracy_scratch, accuracy_sklearn) else 'From Scratch' if accuracy_scratch == max(accuracy_nn, accuracy_scratch, accuracy_sklearn) else 'Sklearn'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
},
  {
   "cell_type": "markdown",
   "id": "52055be8",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red;\">Section 2: Neural network avec tensorflow</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207be9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7a611a",
   "metadata": {},
   "source": [
    "<h2>2) Modèle de réseau de neurones</h2>\n",
    "\n",
    "### Architecture pour la classification:\n",
    "- **Inputs**: 8 features (du dataset diabetes)\n",
    "- **Couches cachées**: 0 (réseau simple)\n",
    "- **Couche de sortie**: 1 neurone (classification binaire)\n",
    "- **Fonction d'activation**: sigmoid (pour probabilités entre 0 et 1)\n",
    "- **Nombre de paramètres**: 8 poids + 1 biais = 9 paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du modèle\n",
    "model_nn = Sequential()\n",
    "output_layer = Dense(1, input_shape=(X_train.shape[1],), activation='sigmoid')\n",
    "model_nn.add(output_layer)\n",
    "\n",
    "# Configuration de l'optimiseur\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "# Compilation du modèle\n",
    "model_nn.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Affichage du résumé du modèle\n",
    "model_nn.summary()\n",
    "\n",
    "# Entraînement du modèle\n",
    "history = model_nn.fit(X_train, y_train, epochs=1000, verbose=0, validation_split=0.2)\n",
    "\n",
    "print(\"\\nEntraînement terminé!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "function_fit_explanation",
   "metadata": {},
   "source": [
    "### Que fait la fonction fit()?\n",
    "\n",
    "Pour la classification, la fonction `fit()` :\n",
    "1. **Minimise la cross-entropie** (fonction de coût pour classification)\n",
    "2. **Ajuste les poids** pour maximiser la probabilité des bonnes classes\n",
    "3. **Utilise la rétropropagation** pour calculer les gradients\n",
    "4. **Optimise avec Adam** pour une convergence efficace\n",
    "5. **Évalue l'accuracy** comme métrique de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "params_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul manuel du nombre de paramètres\n",
    "print(\"=== ANALYSE DES PARAMÈTRES ===\")\n",
    "print(f\"Nombre d'inputs: {X_train.shape[1]}\")\n",
    "print(f\"Nombre de neurones en sortie: 1\")\n",
    "print(f\"Nombre de poids: {X_train.shape[1]} × 1 = {X_train.shape[1]}\")\n",
    "print(f\"Nombre de biais: 1\")\n",
    "print(f\"Total paramètres (calcul manuel): {X_train.shape[1] + 1}\")\n",
    "print(f\"Total paramètres (modèle): {model_nn.count_params()}\")\n",
    "\n",
    "# Extraction des paramètres\n",
    "W_nn, bias_nn = model_nn.layers[0].get_weights()\n",
    "print(f\"\\nPoids (W): {W_nn.flatten()}\")\n",
    "print(f\"Biais (b): {bias_nn}\")\n",
    "\n",
    "# Visualisation de l'entraînement\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Évolution de la Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Binary Crossentropy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Évolution de l\\'Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuning_importance",
   "metadata": {},
   "source": [
    "### Importance du tuning dans la classification\n",
    "\n",
    "Le tuning est crucial pour :\n",
    "1. **Classes déséquilibrées** : Ajuster les poids des classes\n",
    "2. **Seuil de décision** : Optimiser le seuil de classification (0.5 par défaut)\n",
    "3. **Métriques métier** : Choisir entre precision, recall, F1-score\n",
    "4. **Regularisation** : Éviter le surapprentissage avec dropout, L1/L2\n",
    "5. **Learning rate** : Équilibrer vitesse et stabilité de convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851cd5cd",
   "metadata": {},
   "source": [
    "<h2>3) Prédiction en utilisant le modèle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd76e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédictions avec le modèle (probabilités)\n",
    "yhat_nn_proba = model_nn.predict(X_test)\n",
    "yhat_nn = yhat_nn_proba.flatten()\n",
    "\n",
    "# Conversion en classes (seuil = 0.5)\n",
    "yhat_nn_classes = (yhat_nn > 0.5).astype(int)\n",
    "\n",
    "print(f\"Forme des prédictions: {yhat_nn.shape}\")\n",
    "print(f\"Probabilités moyennes: {yhat_nn.mean():.3f}\")\n",
    "print(f\"Premières probabilités: {yhat_nn[:5]}\")\n",
    "print(f\"Premières classes prédites: {yhat_nn_classes[:5]}\")\n",
    "print(f\"Vraies classes: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual_prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction manuelle avec matrices (sans predict())\n",
    "def sigmoid(z):\n",
    "    \"\"\"Fonction sigmoid pour la classification\"\"\"\n",
    "    # Clip z pour éviter overflow\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Extraction des paramètres\n",
    "W_manual, bias_manual = model_nn.layers[0].get_weights()\n",
    "\n",
    "# Calcul manuel: z = X @ W + b, puis sigmoid(z)\n",
    "z_manual = X_test @ W_manual.flatten() + bias_manual[0]\n",
    "yhat_manual = sigmoid(z_manual)\n",
    "\n",
    "print(\"=== COMPARAISON PRÉDICTIONS ===\")\n",
    "print(f\"Prédictions model.predict(): {yhat_nn[:3]}\")\n",
    "print(f\"Prédictions manuelles: {yhat_manual[:3]}\")\n",
    "print(f\"Différence maximale: {np.max(np.abs(yhat_nn - yhat_manual))}\")\n",
    "\n",
    "# Vérification de l'équation du neurone\n",
    "print(f\"\\n=== VÉRIFICATION ÉQUATION NEURONE ===\")\n",
    "print(f\"z = X @ W + b\")\n",
    "print(f\"output = sigmoid(z)\")\n",
    "print(f\"Pour le premier échantillon:\")\n",
    "print(f\"X[0]: {X_test[0]}\")\n",
    "print(f\"z[0]: {z_manual[0]:.6f}\")\n",
    "print(f\"sigmoid(z[0]): {yhat_manual[0]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503925c",
   "metadata": {},
   "source": [
    "<h2>4) Evaluation du modèle</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f13a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance sur le test set\n",
    "accuracy_nn = accuracy_score(y_test, yhat_nn_classes)\n",
    "recall_nn = recall_score(y_test, yhat_nn_classes)\n",
    "f1_nn = f1_score(y_test, yhat_nn_classes)\n",
    "\n",
    "print(\"=== PERFORMANCE MODÈLE NEURAL ===\")\n",
    "print(f\"Accuracy: {accuracy_nn:.4f}\")\n",
    "print(f\"Recall: {recall_nn:.4f}\")\n",
    "print(f\"F1-Score: {f1_nn:.4f}\")\n",
    "\n",
    "# Performance sur le training set\n",
    "yhat_train_proba = model_nn.predict(X_train).flatten()\n",
    "yhat_train_classes = (yhat_train_proba > 0.5).astype(int)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train, yhat_train_classes)\n",
    "recall_train = recall_score(y_train, yhat_train_classes)\n",
    "f1_train = f1_score(y_train, yhat_train_classes)\n",
    "\n",
    "print(f\"\\nAccuracy Training: {accuracy_train:.4f}\")\n",
    "print(f\"Recall Training: {recall_train:.4f}\")\n",
    "print(f\"F1-Score Training: {f1_train:.4f}\")\n",
    "\n",
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, yhat_nn_classes)\n",
    "print(f\"\\nMatrice de confusion:\")\n",
    "print(cm)\n",
    "\n",
    "# Classification report détaillé\n",
    "print(f\"\\nRapport de classification:\")\n",
    "print(classification_report(y_test, yhat_nn_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Matrice de confusion\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Matrice de Confusion')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(2)\n",
    "plt.xticks(tick_marks, ['Non-Diabète', 'Diabète'])\n",
    "plt.yticks(tick_marks, ['Non-Diabète', 'Diabète'])\n",
    "plt.ylabel('Vraie classe')\n",
    "plt.xlabel('Classe prédite')\n",
    "\n",
    "# Ajouter les valeurs dans la matrice\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# 2. Distribution des probabilités prédites\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(yhat_nn[y_test == 0], bins=20, alpha=0.7, label='Non-Diabète', color='blue')\n",
    "plt.hist(yhat_nn[y_test == 1], bins=20, alpha=0.7, label='Diabète', color='red')\n",
    "plt.axvline(0.5, color='black', linestyle='--', label='Seuil')\n",
    "plt.xlabel('Probabilité prédite')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.title('Distribution des Probabilités')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_test, yhat_nn)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de Faux Positifs')\n",
    "plt.ylabel('Taux de Vrais Positifs')\n",
    "plt.title('Courbe ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 4. Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, yhat_nn)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.plot(recall, precision, color='blue', lw=2)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Courbe Precision-Recall')\n",
    "plt.grid(True)\n",
    "\n",
    "# 5. Feature Importance (poids absolus)\n",
    "feature_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
    "                'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "weights_abs = np.abs(W_nn.flatten())\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "indices = np.argsort(weights_abs)[::-1]\n",
    "plt.bar(range(len(weights_abs)), weights_abs[indices])\n",
    "plt.xticks(range(len(weights_abs)), [feature_names[i] for i in indices], rotation=45)\n",
    "plt.title('Importance des Features (|poids|)')\n",
    "plt.ylabel('Valeur absolue du poids')\n",
    "plt.grid(True)\n",
    "\n",
    "# 6. Erreurs de classification\n",
    "plt.subplot(2, 3, 6)\n",
    "errors = (yhat_nn_classes != y_test)\n",
    "plt.scatter(range(len(y_test)), yhat_nn, c=errors, cmap='coolwarm', alpha=0.6)\n",
    "plt.axhline(0.5, color='black', linestyle='--')\n",
    "plt.xlabel('Échantillon')\n",
    "plt.ylabel('Probabilité prédite')\n",
    "plt.title('Erreurs de Classification')\n",
    "plt.colorbar(label='Erreur')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpretation_results",
   "metadata": {},
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "1. **Matrice de confusion** : Montre les vrais/faux positifs et négatifs\n",
    "2. **Accuracy élevée** : Le modèle classe correctement la majorité des cas\n",
    "3. **Recall vs Precision** : Trade-off entre détecter tous les diabétiques vs éviter les faux positifs\n",
    "4. **AUC proche de 1** : Excellente capacité de discrimination\n",
    "5. **Features importantes** : Glucose et BMI semblent les plus discriminants\n",
    "\n",
    "### Pourquoi évaluer sur training ET test set ?\n",
    "- **Surapprentissage** : Si accuracy train >> test, le modèle mémorise\n",
    "- **Généralisation** : Performance similaire = bon modèle\n",
    "- **Robustesse** : Vérifier la stabilité sur différents échantillons\n",
    "- **Sélection de seuil** : Optimiser le trade-off precision/recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d59eb7",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red;\">Section 3: Classification from scratch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016264c",
   "metadata": {},
   "source": [
    "<h2>Modèle de régression logistique from scratch avec utilisation des matrices</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why_from_scratch",
   "metadata": {},
   "source": [
    "### Pourquoi développer des modèles sans librairies ?\n",
    "\n",
    "1. **Compréhension profonde** : Comprendre les mécanismes internes\n",
    "2. **Personnalisation** : Adapter les algorithmes aux besoins spécifiques\n",
    "3. **Optimisation** : Améliorer les performances pour des cas particuliers\n",
    "4. **Debugging** : Identifier et résoudre les problèmes plus facilement\n",
    "5. **Innovation** : Développer de nouveaux algorithmes\n",
    "\n",
    "### Principales étapes :\n",
    "\n",
    "1. **Initialisation** : Paramètres aléatoires (W, b)\n",
    "2. **Forward Pass** : Calcul des prédictions\n",
    "3. **Loss Computation** : Calcul de la fonction de coût\n",
    "4. **Backward Pass** : Calcul des gradients\n",
    "5. **Parameter Update** : Mise à jour par descente de gradient\n",
    "6. **Répétition** : Itérer jusqu'à convergence\n",
    "\n",
    "### Rôle des dérivées :\n",
    "\n",
    "Les dérivées indiquent la **direction et l'ampleur** des changements à apporter aux paramètres :\n",
    "- **Gradient positif** : Diminuer le paramètre\n",
    "- **Gradient négatif** : Augmenter le paramètre\n",
    "- **Gradient proche de 0** : Paramètre optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logistic_from_scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Régression logistique from scratch\n",
    "print(\"=== RÉGRESSION LOGISTIQUE FROM SCRATCH ===\")\n",
    "\n",
    "# Fonction sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\"Fonction d'activation sigmoid\"\"\"\n",
    "    z = np.clip(z, -500, 500)  # Éviter l'overflow\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Fonction de coût (log-likelihood)\n",
    "def compute_cost(y_true, y_pred):\n",
    "    \"\"\"Calcul de la cross-entropy\"\"\"\n",
    "    m = len(y_true)\n",
    "    # Éviter log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    cost = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return cost\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "# Normalisation des données (important pour la régression logistique)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialisation des paramètres\n",
    "np.random.seed(42)\n",
    "W = np.random.normal(0, 0.01, (X_train_scaled.shape[1], 1))  # Shape: (8, 1)\n",
    "b = 0.0\n",
    "\n",
    "# Reshape y pour les calculs matriciels\n",
    "y_train_reshaped = y_train.reshape(-1, 1)\n",
    "m = len(X_train_scaled)\n",
    "\n",
    "# Stockage des coûts et accuracy pour visualisation\n",
    "costs = []\n",
    "accuracies = []\n",
    "\n",
    "print(f\"Forme X_train: {X_train_scaled.shape}\")\n",
    "print(f\"Forme W: {W.shape}\")\n",
    "print(f\"Forme y_train: {y_train_reshaped.shape}\")\n",
    "print(f\"\\nDébut de l'entraînement...\")\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z = X_train_scaled @ W + b  # Linear combination\n",
    "    a = sigmoid(z)  # Activation\n",
    "    \n",
    "    # Calcul du coût\n",
    "    cost = compute_cost(y_train_reshaped, a)\n",
    "    costs.append(cost)\n",
    "    \n",
    "    # Calcul de l'accuracy\n",
    "    predictions = (a > 0.5).astype(int)\n",
    "    accuracy = np.mean(predictions == y_train_reshaped)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Backward pass (calcul des gradients)\n",
    "    dz = a - y_train_reshaped  # Dérivée de la loss par rapport à z\n",
    "    dW = (1/m) * (X_train_scaled.T @ dz)  # Gradient des poids\n",
    "    db = (1/m) * np.sum(dz)  # Gradient du biais\n",
    "    \n",
    "    # Mise à jour des paramètres\n",
    "    W -= learning_rate * dW\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    # Affichage périodique\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Cost: {cost:.6f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nEntraînement terminé!\")\n",
    "print(f\"Cost final: {costs[-1]:.6f}\")\n",
    "print(f\"Accuracy finale: {accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation du modèle from scratch\n",
    "print(\"=== ÉVALUATION MODÈLE FROM SCRATCH ===\")\n",
    "\n",
    "# Prédictions sur le test set\n",
    "z_test = X_test_scaled @ W + b\n",
    "yhat_scratch_proba = sigmoid(z_test).flatten()\n",
    "yhat_scratch_classes = (yhat_scratch_proba > 0.5).astype(int)\n",
    "\n",
    "# Métriques\n",
    "accuracy_scratch = accuracy_score(y_test, yhat_scratch_classes)\n",
    "recall_scratch = recall_score(y_test, yhat_scratch_classes)\n",
    "f1_scratch = f1_score(y_test, yhat_scratch_classes)\n",
    "\n",
    "print(f\"Accuracy (from scratch): {accuracy_scratch:.4f}\")\n",
    "print(f\"Recall (from scratch): {recall_scratch:.4f}\"{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5213bb4",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red;\">Section 1: Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e8be6f",
   "metadata": {},
   "source": [
    "<h2>1) Préparation de données</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f748d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du dataset diabetes\n",
    "dataset = pd.read_csv('diabetes.csv')  # import\n",
    "\n",
    "print(\"Dataset shape:\", dataset.shape)\n",
    "print(\"\\nPremières lignes:\")\n",
    "print(dataset.head())\n",
    "print(\"\\nInformations sur le dataset:\")\n",
    "print(dataset.info())\n",
    "print(\"\\nDistribution de la target:\")\n",
    "print(dataset['Outcome'].value_counts())\n",
    "\n",
    "# Extraction des features et target\n",
    "X = np.array(dataset.drop(columns=['Outcome']))  # features\n",
    "y = np.array(dataset['Outcome'])  # target\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Features: {dataset.columns[:-1].tolist()}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"